{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 4.0,
  "eval_steps": 500,
  "global_step": 2116,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.01890359168241966,
      "grad_norm": 28.180315017700195,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 0.6341,
      "step": 10
    },
    {
      "epoch": 0.03780718336483932,
      "grad_norm": 18.70005989074707,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.5983,
      "step": 20
    },
    {
      "epoch": 0.05671077504725898,
      "grad_norm": 13.89199447631836,
      "learning_rate": 3e-06,
      "loss": 0.4795,
      "step": 30
    },
    {
      "epoch": 0.07561436672967864,
      "grad_norm": 11.116958618164062,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.4591,
      "step": 40
    },
    {
      "epoch": 0.0945179584120983,
      "grad_norm": 8.489206314086914,
      "learning_rate": 5e-06,
      "loss": 0.5472,
      "step": 50
    },
    {
      "epoch": 0.11342155009451796,
      "grad_norm": 30.01156997680664,
      "learning_rate": 6e-06,
      "loss": 0.3714,
      "step": 60
    },
    {
      "epoch": 0.1323251417769376,
      "grad_norm": 9.42944622039795,
      "learning_rate": 7.000000000000001e-06,
      "loss": 0.5631,
      "step": 70
    },
    {
      "epoch": 0.15122873345935728,
      "grad_norm": 5.467212677001953,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.5126,
      "step": 80
    },
    {
      "epoch": 0.17013232514177692,
      "grad_norm": 4.847497940063477,
      "learning_rate": 9e-06,
      "loss": 0.5141,
      "step": 90
    },
    {
      "epoch": 0.1890359168241966,
      "grad_norm": 22.756942749023438,
      "learning_rate": 1e-05,
      "loss": 0.4264,
      "step": 100
    },
    {
      "epoch": 0.20793950850661624,
      "grad_norm": 11.348257064819336,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 0.7025,
      "step": 110
    },
    {
      "epoch": 0.22684310018903592,
      "grad_norm": 10.497285842895508,
      "learning_rate": 1.2e-05,
      "loss": 0.4784,
      "step": 120
    },
    {
      "epoch": 0.24574669187145556,
      "grad_norm": 6.576066970825195,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 0.4198,
      "step": 130
    },
    {
      "epoch": 0.2646502835538752,
      "grad_norm": 13.068428993225098,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 0.5246,
      "step": 140
    },
    {
      "epoch": 0.2835538752362949,
      "grad_norm": 13.612130165100098,
      "learning_rate": 1.5e-05,
      "loss": 0.5183,
      "step": 150
    },
    {
      "epoch": 0.30245746691871456,
      "grad_norm": 6.494453430175781,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.434,
      "step": 160
    },
    {
      "epoch": 0.32136105860113423,
      "grad_norm": 17.02464485168457,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 0.5123,
      "step": 170
    },
    {
      "epoch": 0.34026465028355385,
      "grad_norm": 18.735382080078125,
      "learning_rate": 1.8e-05,
      "loss": 0.3531,
      "step": 180
    },
    {
      "epoch": 0.3591682419659735,
      "grad_norm": 3.948152780532837,
      "learning_rate": 1.9e-05,
      "loss": 0.3996,
      "step": 190
    },
    {
      "epoch": 0.3780718336483932,
      "grad_norm": 5.888492584228516,
      "learning_rate": 2e-05,
      "loss": 0.4208,
      "step": 200
    },
    {
      "epoch": 0.39697542533081287,
      "grad_norm": 25.23052406311035,
      "learning_rate": 2.1e-05,
      "loss": 0.6196,
      "step": 210
    },
    {
      "epoch": 0.4158790170132325,
      "grad_norm": 15.955464363098145,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.4593,
      "step": 220
    },
    {
      "epoch": 0.43478260869565216,
      "grad_norm": 8.81518840789795,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 0.4772,
      "step": 230
    },
    {
      "epoch": 0.45368620037807184,
      "grad_norm": 42.99515151977539,
      "learning_rate": 2.4e-05,
      "loss": 0.5475,
      "step": 240
    },
    {
      "epoch": 0.4725897920604915,
      "grad_norm": 11.427850723266602,
      "learning_rate": 2.5e-05,
      "loss": 0.4541,
      "step": 250
    },
    {
      "epoch": 0.4914933837429111,
      "grad_norm": 10.662052154541016,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 0.4759,
      "step": 260
    },
    {
      "epoch": 0.5103969754253308,
      "grad_norm": 5.8054585456848145,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 0.463,
      "step": 270
    },
    {
      "epoch": 0.5293005671077504,
      "grad_norm": 11.557731628417969,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.5748,
      "step": 280
    },
    {
      "epoch": 0.5482041587901701,
      "grad_norm": 3.1422736644744873,
      "learning_rate": 2.9e-05,
      "loss": 0.3359,
      "step": 290
    },
    {
      "epoch": 0.5671077504725898,
      "grad_norm": 7.100794792175293,
      "learning_rate": 3e-05,
      "loss": 0.6056,
      "step": 300
    },
    {
      "epoch": 0.5860113421550095,
      "grad_norm": 10.585675239562988,
      "learning_rate": 3.1e-05,
      "loss": 0.4875,
      "step": 310
    },
    {
      "epoch": 0.6049149338374291,
      "grad_norm": 6.068000316619873,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 0.511,
      "step": 320
    },
    {
      "epoch": 0.6238185255198487,
      "grad_norm": 2.236053228378296,
      "learning_rate": 3.3e-05,
      "loss": 0.4855,
      "step": 330
    },
    {
      "epoch": 0.6427221172022685,
      "grad_norm": 13.05681324005127,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.5434,
      "step": 340
    },
    {
      "epoch": 0.6616257088846881,
      "grad_norm": 4.744758129119873,
      "learning_rate": 3.5e-05,
      "loss": 0.4789,
      "step": 350
    },
    {
      "epoch": 0.6805293005671077,
      "grad_norm": 2.0914804935455322,
      "learning_rate": 3.6e-05,
      "loss": 0.4411,
      "step": 360
    },
    {
      "epoch": 0.6994328922495274,
      "grad_norm": 5.684807777404785,
      "learning_rate": 3.7e-05,
      "loss": 0.4596,
      "step": 370
    },
    {
      "epoch": 0.718336483931947,
      "grad_norm": 1.5501940250396729,
      "learning_rate": 3.8e-05,
      "loss": 0.5722,
      "step": 380
    },
    {
      "epoch": 0.7372400756143668,
      "grad_norm": 8.399958610534668,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 0.5188,
      "step": 390
    },
    {
      "epoch": 0.7561436672967864,
      "grad_norm": 5.943521499633789,
      "learning_rate": 4e-05,
      "loss": 0.4605,
      "step": 400
    },
    {
      "epoch": 0.775047258979206,
      "grad_norm": 3.4589908123016357,
      "learning_rate": 4.1e-05,
      "loss": 0.5402,
      "step": 410
    },
    {
      "epoch": 0.7939508506616257,
      "grad_norm": 7.106705665588379,
      "learning_rate": 4.2e-05,
      "loss": 0.4515,
      "step": 420
    },
    {
      "epoch": 0.8128544423440454,
      "grad_norm": 4.5625810623168945,
      "learning_rate": 4.3e-05,
      "loss": 0.509,
      "step": 430
    },
    {
      "epoch": 0.831758034026465,
      "grad_norm": 9.6602783203125,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 0.4431,
      "step": 440
    },
    {
      "epoch": 0.8506616257088847,
      "grad_norm": 11.765222549438477,
      "learning_rate": 4.5e-05,
      "loss": 0.6062,
      "step": 450
    },
    {
      "epoch": 0.8695652173913043,
      "grad_norm": 3.9246203899383545,
      "learning_rate": 4.600000000000001e-05,
      "loss": 0.6189,
      "step": 460
    },
    {
      "epoch": 0.888468809073724,
      "grad_norm": 4.397661209106445,
      "learning_rate": 4.7e-05,
      "loss": 0.5141,
      "step": 470
    },
    {
      "epoch": 0.9073724007561437,
      "grad_norm": 1.6518418788909912,
      "learning_rate": 4.8e-05,
      "loss": 0.396,
      "step": 480
    },
    {
      "epoch": 0.9262759924385633,
      "grad_norm": 1.3467077016830444,
      "learning_rate": 4.9e-05,
      "loss": 0.415,
      "step": 490
    },
    {
      "epoch": 0.945179584120983,
      "grad_norm": 18.08839225769043,
      "learning_rate": 5e-05,
      "loss": 0.5097,
      "step": 500
    },
    {
      "epoch": 0.9640831758034026,
      "grad_norm": 2.9766814708709717,
      "learning_rate": 4.976689976689977e-05,
      "loss": 0.4596,
      "step": 510
    },
    {
      "epoch": 0.9829867674858223,
      "grad_norm": 10.522555351257324,
      "learning_rate": 4.9533799533799534e-05,
      "loss": 0.5048,
      "step": 520
    },
    {
      "epoch": 1.001890359168242,
      "grad_norm": 12.301260948181152,
      "learning_rate": 4.93006993006993e-05,
      "loss": 0.5278,
      "step": 530
    },
    {
      "epoch": 1.0207939508506616,
      "grad_norm": 3.5241475105285645,
      "learning_rate": 4.9067599067599065e-05,
      "loss": 0.4709,
      "step": 540
    },
    {
      "epoch": 1.0396975425330812,
      "grad_norm": 3.9673688411712646,
      "learning_rate": 4.8834498834498834e-05,
      "loss": 0.571,
      "step": 550
    },
    {
      "epoch": 1.0586011342155008,
      "grad_norm": 3.0562729835510254,
      "learning_rate": 4.86013986013986e-05,
      "loss": 0.4256,
      "step": 560
    },
    {
      "epoch": 1.0775047258979207,
      "grad_norm": 2.007657051086426,
      "learning_rate": 4.836829836829837e-05,
      "loss": 0.4478,
      "step": 570
    },
    {
      "epoch": 1.0964083175803403,
      "grad_norm": 4.265132904052734,
      "learning_rate": 4.813519813519814e-05,
      "loss": 0.3729,
      "step": 580
    },
    {
      "epoch": 1.11531190926276,
      "grad_norm": 1.7630270719528198,
      "learning_rate": 4.7902097902097904e-05,
      "loss": 0.5436,
      "step": 590
    },
    {
      "epoch": 1.1342155009451795,
      "grad_norm": 6.726306915283203,
      "learning_rate": 4.766899766899767e-05,
      "loss": 0.4095,
      "step": 600
    },
    {
      "epoch": 1.1531190926275992,
      "grad_norm": 4.510070323944092,
      "learning_rate": 4.7435897435897435e-05,
      "loss": 0.5463,
      "step": 610
    },
    {
      "epoch": 1.172022684310019,
      "grad_norm": 3.6744022369384766,
      "learning_rate": 4.7202797202797204e-05,
      "loss": 0.5099,
      "step": 620
    },
    {
      "epoch": 1.1909262759924386,
      "grad_norm": 6.301857948303223,
      "learning_rate": 4.696969696969697e-05,
      "loss": 0.4304,
      "step": 630
    },
    {
      "epoch": 1.2098298676748582,
      "grad_norm": 9.055460929870605,
      "learning_rate": 4.6736596736596735e-05,
      "loss": 0.4481,
      "step": 640
    },
    {
      "epoch": 1.2287334593572778,
      "grad_norm": 3.454441785812378,
      "learning_rate": 4.6503496503496505e-05,
      "loss": 0.4657,
      "step": 650
    },
    {
      "epoch": 1.2476370510396975,
      "grad_norm": 3.4934799671173096,
      "learning_rate": 4.6270396270396274e-05,
      "loss": 0.4966,
      "step": 660
    },
    {
      "epoch": 1.2665406427221173,
      "grad_norm": 4.2063775062561035,
      "learning_rate": 4.603729603729604e-05,
      "loss": 0.4745,
      "step": 670
    },
    {
      "epoch": 1.285444234404537,
      "grad_norm": 1.540030598640442,
      "learning_rate": 4.5804195804195805e-05,
      "loss": 0.3767,
      "step": 680
    },
    {
      "epoch": 1.3043478260869565,
      "grad_norm": 1.5102111101150513,
      "learning_rate": 4.5571095571095574e-05,
      "loss": 0.4955,
      "step": 690
    },
    {
      "epoch": 1.3232514177693762,
      "grad_norm": 9.48978042602539,
      "learning_rate": 4.533799533799534e-05,
      "loss": 0.5473,
      "step": 700
    },
    {
      "epoch": 1.3421550094517958,
      "grad_norm": 4.218029975891113,
      "learning_rate": 4.5104895104895105e-05,
      "loss": 0.5837,
      "step": 710
    },
    {
      "epoch": 1.3610586011342156,
      "grad_norm": 3.078134059906006,
      "learning_rate": 4.4871794871794874e-05,
      "loss": 0.4556,
      "step": 720
    },
    {
      "epoch": 1.3799621928166352,
      "grad_norm": 6.078334808349609,
      "learning_rate": 4.463869463869464e-05,
      "loss": 0.4982,
      "step": 730
    },
    {
      "epoch": 1.3988657844990549,
      "grad_norm": 15.449082374572754,
      "learning_rate": 4.4405594405594406e-05,
      "loss": 0.4433,
      "step": 740
    },
    {
      "epoch": 1.4177693761814745,
      "grad_norm": 1.2826087474822998,
      "learning_rate": 4.4172494172494175e-05,
      "loss": 0.5726,
      "step": 750
    },
    {
      "epoch": 1.436672967863894,
      "grad_norm": 6.571488380432129,
      "learning_rate": 4.3939393939393944e-05,
      "loss": 0.5509,
      "step": 760
    },
    {
      "epoch": 1.455576559546314,
      "grad_norm": 6.11830472946167,
      "learning_rate": 4.370629370629371e-05,
      "loss": 0.3738,
      "step": 770
    },
    {
      "epoch": 1.4744801512287333,
      "grad_norm": 4.752811431884766,
      "learning_rate": 4.3473193473193475e-05,
      "loss": 0.4444,
      "step": 780
    },
    {
      "epoch": 1.4933837429111532,
      "grad_norm": 4.125759601593018,
      "learning_rate": 4.3240093240093244e-05,
      "loss": 0.4973,
      "step": 790
    },
    {
      "epoch": 1.5122873345935728,
      "grad_norm": 4.825638294219971,
      "learning_rate": 4.300699300699301e-05,
      "loss": 0.5217,
      "step": 800
    },
    {
      "epoch": 1.5311909262759924,
      "grad_norm": 3.1402363777160645,
      "learning_rate": 4.2773892773892776e-05,
      "loss": 0.5259,
      "step": 810
    },
    {
      "epoch": 1.5500945179584122,
      "grad_norm": 5.9964141845703125,
      "learning_rate": 4.254079254079254e-05,
      "loss": 0.4041,
      "step": 820
    },
    {
      "epoch": 1.5689981096408316,
      "grad_norm": 1.508156418800354,
      "learning_rate": 4.230769230769231e-05,
      "loss": 0.5,
      "step": 830
    },
    {
      "epoch": 1.5879017013232515,
      "grad_norm": 2.2417421340942383,
      "learning_rate": 4.2074592074592076e-05,
      "loss": 0.5237,
      "step": 840
    },
    {
      "epoch": 1.606805293005671,
      "grad_norm": 7.557713985443115,
      "learning_rate": 4.1841491841491845e-05,
      "loss": 0.4562,
      "step": 850
    },
    {
      "epoch": 1.6257088846880907,
      "grad_norm": 3.7554588317871094,
      "learning_rate": 4.1608391608391614e-05,
      "loss": 0.5469,
      "step": 860
    },
    {
      "epoch": 1.6446124763705106,
      "grad_norm": 1.824650764465332,
      "learning_rate": 4.1375291375291377e-05,
      "loss": 0.5288,
      "step": 870
    },
    {
      "epoch": 1.66351606805293,
      "grad_norm": 3.647751569747925,
      "learning_rate": 4.1142191142191146e-05,
      "loss": 0.5687,
      "step": 880
    },
    {
      "epoch": 1.6824196597353498,
      "grad_norm": 7.244884967803955,
      "learning_rate": 4.0909090909090915e-05,
      "loss": 0.5113,
      "step": 890
    },
    {
      "epoch": 1.7013232514177694,
      "grad_norm": 2.5205845832824707,
      "learning_rate": 4.067599067599068e-05,
      "loss": 0.4478,
      "step": 900
    },
    {
      "epoch": 1.720226843100189,
      "grad_norm": 6.005762577056885,
      "learning_rate": 4.0442890442890446e-05,
      "loss": 0.5162,
      "step": 910
    },
    {
      "epoch": 1.7391304347826086,
      "grad_norm": 1.5589760541915894,
      "learning_rate": 4.020979020979021e-05,
      "loss": 0.4028,
      "step": 920
    },
    {
      "epoch": 1.7580340264650283,
      "grad_norm": 1.5642635822296143,
      "learning_rate": 3.997668997668998e-05,
      "loss": 0.4451,
      "step": 930
    },
    {
      "epoch": 1.776937618147448,
      "grad_norm": 8.885052680969238,
      "learning_rate": 3.974358974358974e-05,
      "loss": 0.5706,
      "step": 940
    },
    {
      "epoch": 1.7958412098298677,
      "grad_norm": 6.953721523284912,
      "learning_rate": 3.9510489510489516e-05,
      "loss": 0.559,
      "step": 950
    },
    {
      "epoch": 1.8147448015122873,
      "grad_norm": 2.0652401447296143,
      "learning_rate": 3.9277389277389285e-05,
      "loss": 0.3924,
      "step": 960
    },
    {
      "epoch": 1.833648393194707,
      "grad_norm": 5.406424522399902,
      "learning_rate": 3.904428904428905e-05,
      "loss": 0.3882,
      "step": 970
    },
    {
      "epoch": 1.8525519848771266,
      "grad_norm": 5.289506912231445,
      "learning_rate": 3.8811188811188816e-05,
      "loss": 0.5024,
      "step": 980
    },
    {
      "epoch": 1.8714555765595464,
      "grad_norm": 4.752135753631592,
      "learning_rate": 3.857808857808858e-05,
      "loss": 0.3323,
      "step": 990
    },
    {
      "epoch": 1.8903591682419658,
      "grad_norm": 4.641644477844238,
      "learning_rate": 3.834498834498835e-05,
      "loss": 0.6206,
      "step": 1000
    },
    {
      "epoch": 1.9092627599243857,
      "grad_norm": 1.4778504371643066,
      "learning_rate": 3.811188811188811e-05,
      "loss": 0.4234,
      "step": 1010
    },
    {
      "epoch": 1.9281663516068053,
      "grad_norm": 5.170569896697998,
      "learning_rate": 3.787878787878788e-05,
      "loss": 0.4206,
      "step": 1020
    },
    {
      "epoch": 1.947069943289225,
      "grad_norm": 4.539720058441162,
      "learning_rate": 3.764568764568765e-05,
      "loss": 0.5975,
      "step": 1030
    },
    {
      "epoch": 1.9659735349716447,
      "grad_norm": 2.5511326789855957,
      "learning_rate": 3.741258741258741e-05,
      "loss": 0.6142,
      "step": 1040
    },
    {
      "epoch": 1.9848771266540641,
      "grad_norm": 8.227049827575684,
      "learning_rate": 3.717948717948718e-05,
      "loss": 0.5384,
      "step": 1050
    },
    {
      "epoch": 2.003780718336484,
      "grad_norm": 7.5241379737854,
      "learning_rate": 3.694638694638695e-05,
      "loss": 0.4771,
      "step": 1060
    },
    {
      "epoch": 2.022684310018904,
      "grad_norm": 2.2721128463745117,
      "learning_rate": 3.671328671328672e-05,
      "loss": 0.4853,
      "step": 1070
    },
    {
      "epoch": 2.041587901701323,
      "grad_norm": 2.8737659454345703,
      "learning_rate": 3.6480186480186486e-05,
      "loss": 0.4521,
      "step": 1080
    },
    {
      "epoch": 2.060491493383743,
      "grad_norm": 6.2552714347839355,
      "learning_rate": 3.624708624708625e-05,
      "loss": 0.5255,
      "step": 1090
    },
    {
      "epoch": 2.0793950850661624,
      "grad_norm": 1.6439292430877686,
      "learning_rate": 3.601398601398602e-05,
      "loss": 0.3848,
      "step": 1100
    },
    {
      "epoch": 2.0982986767485823,
      "grad_norm": 5.129034042358398,
      "learning_rate": 3.578088578088578e-05,
      "loss": 0.3423,
      "step": 1110
    },
    {
      "epoch": 2.1172022684310017,
      "grad_norm": 5.4259490966796875,
      "learning_rate": 3.554778554778555e-05,
      "loss": 0.418,
      "step": 1120
    },
    {
      "epoch": 2.1361058601134215,
      "grad_norm": 8.963140487670898,
      "learning_rate": 3.531468531468531e-05,
      "loss": 0.749,
      "step": 1130
    },
    {
      "epoch": 2.1550094517958414,
      "grad_norm": 2.829366445541382,
      "learning_rate": 3.508158508158508e-05,
      "loss": 0.4509,
      "step": 1140
    },
    {
      "epoch": 2.1739130434782608,
      "grad_norm": 6.00026273727417,
      "learning_rate": 3.484848484848485e-05,
      "loss": 0.3712,
      "step": 1150
    },
    {
      "epoch": 2.1928166351606806,
      "grad_norm": 4.984893321990967,
      "learning_rate": 3.461538461538462e-05,
      "loss": 0.4228,
      "step": 1160
    },
    {
      "epoch": 2.2117202268431,
      "grad_norm": 1.4593100547790527,
      "learning_rate": 3.438228438228439e-05,
      "loss": 0.4857,
      "step": 1170
    },
    {
      "epoch": 2.23062381852552,
      "grad_norm": 3.905733823776245,
      "learning_rate": 3.414918414918415e-05,
      "loss": 0.4909,
      "step": 1180
    },
    {
      "epoch": 2.2495274102079397,
      "grad_norm": 1.7728639841079712,
      "learning_rate": 3.391608391608392e-05,
      "loss": 0.5036,
      "step": 1190
    },
    {
      "epoch": 2.268431001890359,
      "grad_norm": 2.179745674133301,
      "learning_rate": 3.368298368298368e-05,
      "loss": 0.6536,
      "step": 1200
    },
    {
      "epoch": 2.287334593572779,
      "grad_norm": 6.678843021392822,
      "learning_rate": 3.344988344988345e-05,
      "loss": 0.5116,
      "step": 1210
    },
    {
      "epoch": 2.3062381852551983,
      "grad_norm": 3.265346050262451,
      "learning_rate": 3.321678321678322e-05,
      "loss": 0.5412,
      "step": 1220
    },
    {
      "epoch": 2.325141776937618,
      "grad_norm": 2.9458069801330566,
      "learning_rate": 3.298368298368298e-05,
      "loss": 0.5403,
      "step": 1230
    },
    {
      "epoch": 2.344045368620038,
      "grad_norm": 2.580137014389038,
      "learning_rate": 3.275058275058275e-05,
      "loss": 0.359,
      "step": 1240
    },
    {
      "epoch": 2.3629489603024574,
      "grad_norm": 2.587937355041504,
      "learning_rate": 3.251748251748252e-05,
      "loss": 0.509,
      "step": 1250
    },
    {
      "epoch": 2.381852551984877,
      "grad_norm": 2.8687615394592285,
      "learning_rate": 3.228438228438229e-05,
      "loss": 0.4898,
      "step": 1260
    },
    {
      "epoch": 2.4007561436672966,
      "grad_norm": 7.373074531555176,
      "learning_rate": 3.205128205128206e-05,
      "loss": 0.5097,
      "step": 1270
    },
    {
      "epoch": 2.4196597353497165,
      "grad_norm": 7.849364757537842,
      "learning_rate": 3.181818181818182e-05,
      "loss": 0.5095,
      "step": 1280
    },
    {
      "epoch": 2.4385633270321363,
      "grad_norm": 6.801962375640869,
      "learning_rate": 3.158508158508159e-05,
      "loss": 0.4723,
      "step": 1290
    },
    {
      "epoch": 2.4574669187145557,
      "grad_norm": 3.472907781600952,
      "learning_rate": 3.135198135198135e-05,
      "loss": 0.4925,
      "step": 1300
    },
    {
      "epoch": 2.4763705103969755,
      "grad_norm": 3.07468581199646,
      "learning_rate": 3.111888111888112e-05,
      "loss": 0.4956,
      "step": 1310
    },
    {
      "epoch": 2.495274102079395,
      "grad_norm": 2.5797338485717773,
      "learning_rate": 3.088578088578088e-05,
      "loss": 0.4509,
      "step": 1320
    },
    {
      "epoch": 2.5141776937618148,
      "grad_norm": 2.0313329696655273,
      "learning_rate": 3.065268065268065e-05,
      "loss": 0.5323,
      "step": 1330
    },
    {
      "epoch": 2.5330812854442346,
      "grad_norm": 4.313287734985352,
      "learning_rate": 3.0419580419580425e-05,
      "loss": 0.4343,
      "step": 1340
    },
    {
      "epoch": 2.551984877126654,
      "grad_norm": 4.564220428466797,
      "learning_rate": 3.0186480186480187e-05,
      "loss": 0.4679,
      "step": 1350
    },
    {
      "epoch": 2.570888468809074,
      "grad_norm": 5.575353622436523,
      "learning_rate": 2.9953379953379956e-05,
      "loss": 0.4221,
      "step": 1360
    },
    {
      "epoch": 2.5897920604914932,
      "grad_norm": 1.7099931240081787,
      "learning_rate": 2.972027972027972e-05,
      "loss": 0.5318,
      "step": 1370
    },
    {
      "epoch": 2.608695652173913,
      "grad_norm": 1.5604268312454224,
      "learning_rate": 2.948717948717949e-05,
      "loss": 0.4226,
      "step": 1380
    },
    {
      "epoch": 2.6275992438563325,
      "grad_norm": 5.833178520202637,
      "learning_rate": 2.9254079254079253e-05,
      "loss": 0.5027,
      "step": 1390
    },
    {
      "epoch": 2.6465028355387523,
      "grad_norm": 1.941444754600525,
      "learning_rate": 2.9020979020979022e-05,
      "loss": 0.4033,
      "step": 1400
    },
    {
      "epoch": 2.665406427221172,
      "grad_norm": 3.992398500442505,
      "learning_rate": 2.878787878787879e-05,
      "loss": 0.6169,
      "step": 1410
    },
    {
      "epoch": 2.6843100189035916,
      "grad_norm": 6.072514533996582,
      "learning_rate": 2.8554778554778557e-05,
      "loss": 0.4886,
      "step": 1420
    },
    {
      "epoch": 2.7032136105860114,
      "grad_norm": 7.324105739593506,
      "learning_rate": 2.8321678321678326e-05,
      "loss": 0.5599,
      "step": 1430
    },
    {
      "epoch": 2.7221172022684312,
      "grad_norm": 3.4047563076019287,
      "learning_rate": 2.8088578088578088e-05,
      "loss": 0.4632,
      "step": 1440
    },
    {
      "epoch": 2.7410207939508506,
      "grad_norm": 2.3554728031158447,
      "learning_rate": 2.7855477855477857e-05,
      "loss": 0.4758,
      "step": 1450
    },
    {
      "epoch": 2.7599243856332705,
      "grad_norm": 3.9471609592437744,
      "learning_rate": 2.762237762237762e-05,
      "loss": 0.4764,
      "step": 1460
    },
    {
      "epoch": 2.77882797731569,
      "grad_norm": 1.4621282815933228,
      "learning_rate": 2.738927738927739e-05,
      "loss": 0.5383,
      "step": 1470
    },
    {
      "epoch": 2.7977315689981097,
      "grad_norm": 8.772844314575195,
      "learning_rate": 2.715617715617716e-05,
      "loss": 0.5543,
      "step": 1480
    },
    {
      "epoch": 2.816635160680529,
      "grad_norm": 2.484523296356201,
      "learning_rate": 2.6923076923076923e-05,
      "loss": 0.4407,
      "step": 1490
    },
    {
      "epoch": 2.835538752362949,
      "grad_norm": 3.9231698513031006,
      "learning_rate": 2.6689976689976692e-05,
      "loss": 0.4313,
      "step": 1500
    },
    {
      "epoch": 2.854442344045369,
      "grad_norm": 4.7417168617248535,
      "learning_rate": 2.6456876456876455e-05,
      "loss": 0.381,
      "step": 1510
    },
    {
      "epoch": 2.873345935727788,
      "grad_norm": 5.111018180847168,
      "learning_rate": 2.6223776223776224e-05,
      "loss": 0.4974,
      "step": 1520
    },
    {
      "epoch": 2.892249527410208,
      "grad_norm": 5.96575927734375,
      "learning_rate": 2.5990675990675993e-05,
      "loss": 0.3834,
      "step": 1530
    },
    {
      "epoch": 2.911153119092628,
      "grad_norm": 2.158494472503662,
      "learning_rate": 2.575757575757576e-05,
      "loss": 0.5478,
      "step": 1540
    },
    {
      "epoch": 2.9300567107750473,
      "grad_norm": 1.6961989402770996,
      "learning_rate": 2.5524475524475528e-05,
      "loss": 0.4022,
      "step": 1550
    },
    {
      "epoch": 2.9489603024574667,
      "grad_norm": 4.769635200500488,
      "learning_rate": 2.529137529137529e-05,
      "loss": 0.4977,
      "step": 1560
    },
    {
      "epoch": 2.9678638941398865,
      "grad_norm": 2.6359379291534424,
      "learning_rate": 2.505827505827506e-05,
      "loss": 0.5718,
      "step": 1570
    },
    {
      "epoch": 2.9867674858223063,
      "grad_norm": 2.88962459564209,
      "learning_rate": 2.4825174825174828e-05,
      "loss": 0.4246,
      "step": 1580
    },
    {
      "epoch": 3.0056710775047257,
      "grad_norm": 3.6404178142547607,
      "learning_rate": 2.4592074592074594e-05,
      "loss": 0.4107,
      "step": 1590
    },
    {
      "epoch": 3.0245746691871456,
      "grad_norm": 5.17860221862793,
      "learning_rate": 2.435897435897436e-05,
      "loss": 0.3347,
      "step": 1600
    },
    {
      "epoch": 3.0434782608695654,
      "grad_norm": 5.444827079772949,
      "learning_rate": 2.4125874125874125e-05,
      "loss": 0.4881,
      "step": 1610
    },
    {
      "epoch": 3.062381852551985,
      "grad_norm": 5.036777973175049,
      "learning_rate": 2.3892773892773894e-05,
      "loss": 0.5739,
      "step": 1620
    },
    {
      "epoch": 3.0812854442344046,
      "grad_norm": 5.912884712219238,
      "learning_rate": 2.3659673659673663e-05,
      "loss": 0.443,
      "step": 1630
    },
    {
      "epoch": 3.100189035916824,
      "grad_norm": 4.149127960205078,
      "learning_rate": 2.342657342657343e-05,
      "loss": 0.415,
      "step": 1640
    },
    {
      "epoch": 3.119092627599244,
      "grad_norm": 13.790910720825195,
      "learning_rate": 2.3193473193473195e-05,
      "loss": 0.4833,
      "step": 1650
    },
    {
      "epoch": 3.1379962192816637,
      "grad_norm": 5.534366607666016,
      "learning_rate": 2.296037296037296e-05,
      "loss": 0.3851,
      "step": 1660
    },
    {
      "epoch": 3.156899810964083,
      "grad_norm": 1.5099680423736572,
      "learning_rate": 2.272727272727273e-05,
      "loss": 0.5694,
      "step": 1670
    },
    {
      "epoch": 3.175803402646503,
      "grad_norm": 4.097660064697266,
      "learning_rate": 2.2494172494172495e-05,
      "loss": 0.4951,
      "step": 1680
    },
    {
      "epoch": 3.1947069943289224,
      "grad_norm": 2.2609763145446777,
      "learning_rate": 2.2261072261072264e-05,
      "loss": 0.6183,
      "step": 1690
    },
    {
      "epoch": 3.213610586011342,
      "grad_norm": 7.913806438446045,
      "learning_rate": 2.202797202797203e-05,
      "loss": 0.4977,
      "step": 1700
    },
    {
      "epoch": 3.232514177693762,
      "grad_norm": 6.830959796905518,
      "learning_rate": 2.1794871794871795e-05,
      "loss": 0.366,
      "step": 1710
    },
    {
      "epoch": 3.2514177693761814,
      "grad_norm": 2.078932762145996,
      "learning_rate": 2.156177156177156e-05,
      "loss": 0.5188,
      "step": 1720
    },
    {
      "epoch": 3.2703213610586013,
      "grad_norm": 7.314085006713867,
      "learning_rate": 2.132867132867133e-05,
      "loss": 0.3914,
      "step": 1730
    },
    {
      "epoch": 3.2892249527410207,
      "grad_norm": 8.128581047058105,
      "learning_rate": 2.1095571095571096e-05,
      "loss": 0.5083,
      "step": 1740
    },
    {
      "epoch": 3.3081285444234405,
      "grad_norm": 1.4936981201171875,
      "learning_rate": 2.0862470862470865e-05,
      "loss": 0.381,
      "step": 1750
    },
    {
      "epoch": 3.32703213610586,
      "grad_norm": 1.6174930334091187,
      "learning_rate": 2.062937062937063e-05,
      "loss": 0.4632,
      "step": 1760
    },
    {
      "epoch": 3.3459357277882797,
      "grad_norm": 5.309018135070801,
      "learning_rate": 2.0396270396270396e-05,
      "loss": 0.3913,
      "step": 1770
    },
    {
      "epoch": 3.3648393194706996,
      "grad_norm": 9.430305480957031,
      "learning_rate": 2.0163170163170165e-05,
      "loss": 0.6126,
      "step": 1780
    },
    {
      "epoch": 3.383742911153119,
      "grad_norm": 5.180495738983154,
      "learning_rate": 1.993006993006993e-05,
      "loss": 0.5625,
      "step": 1790
    },
    {
      "epoch": 3.402646502835539,
      "grad_norm": 5.483678340911865,
      "learning_rate": 1.9696969696969697e-05,
      "loss": 0.4647,
      "step": 1800
    },
    {
      "epoch": 3.421550094517958,
      "grad_norm": 4.781481742858887,
      "learning_rate": 1.9463869463869462e-05,
      "loss": 0.5059,
      "step": 1810
    },
    {
      "epoch": 3.440453686200378,
      "grad_norm": 3.989215135574341,
      "learning_rate": 1.923076923076923e-05,
      "loss": 0.5119,
      "step": 1820
    },
    {
      "epoch": 3.459357277882798,
      "grad_norm": 4.091522216796875,
      "learning_rate": 1.8997668997669e-05,
      "loss": 0.5738,
      "step": 1830
    },
    {
      "epoch": 3.4782608695652173,
      "grad_norm": 3.0266289710998535,
      "learning_rate": 1.8764568764568766e-05,
      "loss": 0.4522,
      "step": 1840
    },
    {
      "epoch": 3.497164461247637,
      "grad_norm": 7.657750129699707,
      "learning_rate": 1.8531468531468532e-05,
      "loss": 0.5143,
      "step": 1850
    },
    {
      "epoch": 3.5160680529300565,
      "grad_norm": 2.713439702987671,
      "learning_rate": 1.8298368298368298e-05,
      "loss": 0.4749,
      "step": 1860
    },
    {
      "epoch": 3.5349716446124764,
      "grad_norm": 7.142196178436279,
      "learning_rate": 1.8065268065268067e-05,
      "loss": 0.4957,
      "step": 1870
    },
    {
      "epoch": 3.553875236294896,
      "grad_norm": 1.7249102592468262,
      "learning_rate": 1.7832167832167836e-05,
      "loss": 0.4375,
      "step": 1880
    },
    {
      "epoch": 3.5727788279773156,
      "grad_norm": 1.6817864179611206,
      "learning_rate": 1.75990675990676e-05,
      "loss": 0.3817,
      "step": 1890
    },
    {
      "epoch": 3.5916824196597354,
      "grad_norm": 6.238315582275391,
      "learning_rate": 1.7365967365967367e-05,
      "loss": 0.3981,
      "step": 1900
    },
    {
      "epoch": 3.610586011342155,
      "grad_norm": 8.92679500579834,
      "learning_rate": 1.7132867132867133e-05,
      "loss": 0.5181,
      "step": 1910
    },
    {
      "epoch": 3.6294896030245747,
      "grad_norm": 5.092813014984131,
      "learning_rate": 1.68997668997669e-05,
      "loss": 0.4195,
      "step": 1920
    },
    {
      "epoch": 3.648393194706994,
      "grad_norm": 5.340569019317627,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 0.4415,
      "step": 1930
    },
    {
      "epoch": 3.667296786389414,
      "grad_norm": 9.547121047973633,
      "learning_rate": 1.6433566433566433e-05,
      "loss": 0.6276,
      "step": 1940
    },
    {
      "epoch": 3.6862003780718338,
      "grad_norm": 8.315391540527344,
      "learning_rate": 1.6200466200466202e-05,
      "loss": 0.4385,
      "step": 1950
    },
    {
      "epoch": 3.705103969754253,
      "grad_norm": 1.8306667804718018,
      "learning_rate": 1.5967365967365968e-05,
      "loss": 0.5179,
      "step": 1960
    },
    {
      "epoch": 3.724007561436673,
      "grad_norm": 4.5109124183654785,
      "learning_rate": 1.5734265734265734e-05,
      "loss": 0.5454,
      "step": 1970
    },
    {
      "epoch": 3.742911153119093,
      "grad_norm": 2.149487257003784,
      "learning_rate": 1.5501165501165503e-05,
      "loss": 0.4849,
      "step": 1980
    },
    {
      "epoch": 3.7618147448015122,
      "grad_norm": 6.356109142303467,
      "learning_rate": 1.526806526806527e-05,
      "loss": 0.4749,
      "step": 1990
    },
    {
      "epoch": 3.780718336483932,
      "grad_norm": 1.931806206703186,
      "learning_rate": 1.5034965034965034e-05,
      "loss": 0.5097,
      "step": 2000
    },
    {
      "epoch": 3.7996219281663515,
      "grad_norm": 23.037649154663086,
      "learning_rate": 1.4801864801864803e-05,
      "loss": 0.4682,
      "step": 2010
    },
    {
      "epoch": 3.8185255198487713,
      "grad_norm": 3.9279048442840576,
      "learning_rate": 1.456876456876457e-05,
      "loss": 0.4533,
      "step": 2020
    },
    {
      "epoch": 3.8374291115311907,
      "grad_norm": 5.210140705108643,
      "learning_rate": 1.4335664335664336e-05,
      "loss": 0.4906,
      "step": 2030
    },
    {
      "epoch": 3.8563327032136105,
      "grad_norm": 5.296356201171875,
      "learning_rate": 1.4102564102564104e-05,
      "loss": 0.5294,
      "step": 2040
    },
    {
      "epoch": 3.8752362948960304,
      "grad_norm": 2.1275312900543213,
      "learning_rate": 1.386946386946387e-05,
      "loss": 0.5615,
      "step": 2050
    },
    {
      "epoch": 3.89413988657845,
      "grad_norm": 6.162741184234619,
      "learning_rate": 1.3636363636363637e-05,
      "loss": 0.478,
      "step": 2060
    },
    {
      "epoch": 3.9130434782608696,
      "grad_norm": 1.442750096321106,
      "learning_rate": 1.3403263403263406e-05,
      "loss": 0.4003,
      "step": 2070
    },
    {
      "epoch": 3.9319470699432895,
      "grad_norm": 1.8849231004714966,
      "learning_rate": 1.3170163170163171e-05,
      "loss": 0.5058,
      "step": 2080
    },
    {
      "epoch": 3.950850661625709,
      "grad_norm": 2.11767578125,
      "learning_rate": 1.2937062937062939e-05,
      "loss": 0.5945,
      "step": 2090
    },
    {
      "epoch": 3.9697542533081287,
      "grad_norm": 3.8159351348876953,
      "learning_rate": 1.2703962703962704e-05,
      "loss": 0.512,
      "step": 2100
    },
    {
      "epoch": 3.988657844990548,
      "grad_norm": 4.456650257110596,
      "learning_rate": 1.2470862470862472e-05,
      "loss": 0.4777,
      "step": 2110
    }
  ],
  "logging_steps": 10,
  "max_steps": 2645,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 404451474186240.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
