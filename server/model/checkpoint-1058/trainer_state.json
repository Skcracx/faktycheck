{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 1058,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.01890359168241966,
      "grad_norm": 28.180315017700195,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 0.6341,
      "step": 10
    },
    {
      "epoch": 0.03780718336483932,
      "grad_norm": 18.70005989074707,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.5983,
      "step": 20
    },
    {
      "epoch": 0.05671077504725898,
      "grad_norm": 13.89199447631836,
      "learning_rate": 3e-06,
      "loss": 0.4795,
      "step": 30
    },
    {
      "epoch": 0.07561436672967864,
      "grad_norm": 11.116958618164062,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.4591,
      "step": 40
    },
    {
      "epoch": 0.0945179584120983,
      "grad_norm": 8.489206314086914,
      "learning_rate": 5e-06,
      "loss": 0.5472,
      "step": 50
    },
    {
      "epoch": 0.11342155009451796,
      "grad_norm": 30.01156997680664,
      "learning_rate": 6e-06,
      "loss": 0.3714,
      "step": 60
    },
    {
      "epoch": 0.1323251417769376,
      "grad_norm": 9.42944622039795,
      "learning_rate": 7.000000000000001e-06,
      "loss": 0.5631,
      "step": 70
    },
    {
      "epoch": 0.15122873345935728,
      "grad_norm": 5.467212677001953,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.5126,
      "step": 80
    },
    {
      "epoch": 0.17013232514177692,
      "grad_norm": 4.847497940063477,
      "learning_rate": 9e-06,
      "loss": 0.5141,
      "step": 90
    },
    {
      "epoch": 0.1890359168241966,
      "grad_norm": 22.756942749023438,
      "learning_rate": 1e-05,
      "loss": 0.4264,
      "step": 100
    },
    {
      "epoch": 0.20793950850661624,
      "grad_norm": 11.348257064819336,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 0.7025,
      "step": 110
    },
    {
      "epoch": 0.22684310018903592,
      "grad_norm": 10.497285842895508,
      "learning_rate": 1.2e-05,
      "loss": 0.4784,
      "step": 120
    },
    {
      "epoch": 0.24574669187145556,
      "grad_norm": 6.576066970825195,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 0.4198,
      "step": 130
    },
    {
      "epoch": 0.2646502835538752,
      "grad_norm": 13.068428993225098,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 0.5246,
      "step": 140
    },
    {
      "epoch": 0.2835538752362949,
      "grad_norm": 13.612130165100098,
      "learning_rate": 1.5e-05,
      "loss": 0.5183,
      "step": 150
    },
    {
      "epoch": 0.30245746691871456,
      "grad_norm": 6.494453430175781,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.434,
      "step": 160
    },
    {
      "epoch": 0.32136105860113423,
      "grad_norm": 17.02464485168457,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 0.5123,
      "step": 170
    },
    {
      "epoch": 0.34026465028355385,
      "grad_norm": 18.735382080078125,
      "learning_rate": 1.8e-05,
      "loss": 0.3531,
      "step": 180
    },
    {
      "epoch": 0.3591682419659735,
      "grad_norm": 3.948152780532837,
      "learning_rate": 1.9e-05,
      "loss": 0.3996,
      "step": 190
    },
    {
      "epoch": 0.3780718336483932,
      "grad_norm": 5.888492584228516,
      "learning_rate": 2e-05,
      "loss": 0.4208,
      "step": 200
    },
    {
      "epoch": 0.39697542533081287,
      "grad_norm": 25.23052406311035,
      "learning_rate": 2.1e-05,
      "loss": 0.6196,
      "step": 210
    },
    {
      "epoch": 0.4158790170132325,
      "grad_norm": 15.955464363098145,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 0.4593,
      "step": 220
    },
    {
      "epoch": 0.43478260869565216,
      "grad_norm": 8.81518840789795,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 0.4772,
      "step": 230
    },
    {
      "epoch": 0.45368620037807184,
      "grad_norm": 42.99515151977539,
      "learning_rate": 2.4e-05,
      "loss": 0.5475,
      "step": 240
    },
    {
      "epoch": 0.4725897920604915,
      "grad_norm": 11.427850723266602,
      "learning_rate": 2.5e-05,
      "loss": 0.4541,
      "step": 250
    },
    {
      "epoch": 0.4914933837429111,
      "grad_norm": 10.662052154541016,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 0.4759,
      "step": 260
    },
    {
      "epoch": 0.5103969754253308,
      "grad_norm": 5.8054585456848145,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 0.463,
      "step": 270
    },
    {
      "epoch": 0.5293005671077504,
      "grad_norm": 11.557731628417969,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 0.5748,
      "step": 280
    },
    {
      "epoch": 0.5482041587901701,
      "grad_norm": 3.1422736644744873,
      "learning_rate": 2.9e-05,
      "loss": 0.3359,
      "step": 290
    },
    {
      "epoch": 0.5671077504725898,
      "grad_norm": 7.100794792175293,
      "learning_rate": 3e-05,
      "loss": 0.6056,
      "step": 300
    },
    {
      "epoch": 0.5860113421550095,
      "grad_norm": 10.585675239562988,
      "learning_rate": 3.1e-05,
      "loss": 0.4875,
      "step": 310
    },
    {
      "epoch": 0.6049149338374291,
      "grad_norm": 6.068000316619873,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 0.511,
      "step": 320
    },
    {
      "epoch": 0.6238185255198487,
      "grad_norm": 2.236053228378296,
      "learning_rate": 3.3e-05,
      "loss": 0.4855,
      "step": 330
    },
    {
      "epoch": 0.6427221172022685,
      "grad_norm": 13.05681324005127,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 0.5434,
      "step": 340
    },
    {
      "epoch": 0.6616257088846881,
      "grad_norm": 4.744758129119873,
      "learning_rate": 3.5e-05,
      "loss": 0.4789,
      "step": 350
    },
    {
      "epoch": 0.6805293005671077,
      "grad_norm": 2.0914804935455322,
      "learning_rate": 3.6e-05,
      "loss": 0.4411,
      "step": 360
    },
    {
      "epoch": 0.6994328922495274,
      "grad_norm": 5.684807777404785,
      "learning_rate": 3.7e-05,
      "loss": 0.4596,
      "step": 370
    },
    {
      "epoch": 0.718336483931947,
      "grad_norm": 1.5501940250396729,
      "learning_rate": 3.8e-05,
      "loss": 0.5722,
      "step": 380
    },
    {
      "epoch": 0.7372400756143668,
      "grad_norm": 8.399958610534668,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 0.5188,
      "step": 390
    },
    {
      "epoch": 0.7561436672967864,
      "grad_norm": 5.943521499633789,
      "learning_rate": 4e-05,
      "loss": 0.4605,
      "step": 400
    },
    {
      "epoch": 0.775047258979206,
      "grad_norm": 3.4589908123016357,
      "learning_rate": 4.1e-05,
      "loss": 0.5402,
      "step": 410
    },
    {
      "epoch": 0.7939508506616257,
      "grad_norm": 7.106705665588379,
      "learning_rate": 4.2e-05,
      "loss": 0.4515,
      "step": 420
    },
    {
      "epoch": 0.8128544423440454,
      "grad_norm": 4.5625810623168945,
      "learning_rate": 4.3e-05,
      "loss": 0.509,
      "step": 430
    },
    {
      "epoch": 0.831758034026465,
      "grad_norm": 9.6602783203125,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 0.4431,
      "step": 440
    },
    {
      "epoch": 0.8506616257088847,
      "grad_norm": 11.765222549438477,
      "learning_rate": 4.5e-05,
      "loss": 0.6062,
      "step": 450
    },
    {
      "epoch": 0.8695652173913043,
      "grad_norm": 3.9246203899383545,
      "learning_rate": 4.600000000000001e-05,
      "loss": 0.6189,
      "step": 460
    },
    {
      "epoch": 0.888468809073724,
      "grad_norm": 4.397661209106445,
      "learning_rate": 4.7e-05,
      "loss": 0.5141,
      "step": 470
    },
    {
      "epoch": 0.9073724007561437,
      "grad_norm": 1.6518418788909912,
      "learning_rate": 4.8e-05,
      "loss": 0.396,
      "step": 480
    },
    {
      "epoch": 0.9262759924385633,
      "grad_norm": 1.3467077016830444,
      "learning_rate": 4.9e-05,
      "loss": 0.415,
      "step": 490
    },
    {
      "epoch": 0.945179584120983,
      "grad_norm": 18.08839225769043,
      "learning_rate": 5e-05,
      "loss": 0.5097,
      "step": 500
    },
    {
      "epoch": 0.9640831758034026,
      "grad_norm": 2.9766814708709717,
      "learning_rate": 4.976689976689977e-05,
      "loss": 0.4596,
      "step": 510
    },
    {
      "epoch": 0.9829867674858223,
      "grad_norm": 10.522555351257324,
      "learning_rate": 4.9533799533799534e-05,
      "loss": 0.5048,
      "step": 520
    },
    {
      "epoch": 1.001890359168242,
      "grad_norm": 12.301260948181152,
      "learning_rate": 4.93006993006993e-05,
      "loss": 0.5278,
      "step": 530
    },
    {
      "epoch": 1.0207939508506616,
      "grad_norm": 3.5241475105285645,
      "learning_rate": 4.9067599067599065e-05,
      "loss": 0.4709,
      "step": 540
    },
    {
      "epoch": 1.0396975425330812,
      "grad_norm": 3.9673688411712646,
      "learning_rate": 4.8834498834498834e-05,
      "loss": 0.571,
      "step": 550
    },
    {
      "epoch": 1.0586011342155008,
      "grad_norm": 3.0562729835510254,
      "learning_rate": 4.86013986013986e-05,
      "loss": 0.4256,
      "step": 560
    },
    {
      "epoch": 1.0775047258979207,
      "grad_norm": 2.007657051086426,
      "learning_rate": 4.836829836829837e-05,
      "loss": 0.4478,
      "step": 570
    },
    {
      "epoch": 1.0964083175803403,
      "grad_norm": 4.265132904052734,
      "learning_rate": 4.813519813519814e-05,
      "loss": 0.3729,
      "step": 580
    },
    {
      "epoch": 1.11531190926276,
      "grad_norm": 1.7630270719528198,
      "learning_rate": 4.7902097902097904e-05,
      "loss": 0.5436,
      "step": 590
    },
    {
      "epoch": 1.1342155009451795,
      "grad_norm": 6.726306915283203,
      "learning_rate": 4.766899766899767e-05,
      "loss": 0.4095,
      "step": 600
    },
    {
      "epoch": 1.1531190926275992,
      "grad_norm": 4.510070323944092,
      "learning_rate": 4.7435897435897435e-05,
      "loss": 0.5463,
      "step": 610
    },
    {
      "epoch": 1.172022684310019,
      "grad_norm": 3.6744022369384766,
      "learning_rate": 4.7202797202797204e-05,
      "loss": 0.5099,
      "step": 620
    },
    {
      "epoch": 1.1909262759924386,
      "grad_norm": 6.301857948303223,
      "learning_rate": 4.696969696969697e-05,
      "loss": 0.4304,
      "step": 630
    },
    {
      "epoch": 1.2098298676748582,
      "grad_norm": 9.055460929870605,
      "learning_rate": 4.6736596736596735e-05,
      "loss": 0.4481,
      "step": 640
    },
    {
      "epoch": 1.2287334593572778,
      "grad_norm": 3.454441785812378,
      "learning_rate": 4.6503496503496505e-05,
      "loss": 0.4657,
      "step": 650
    },
    {
      "epoch": 1.2476370510396975,
      "grad_norm": 3.4934799671173096,
      "learning_rate": 4.6270396270396274e-05,
      "loss": 0.4966,
      "step": 660
    },
    {
      "epoch": 1.2665406427221173,
      "grad_norm": 4.2063775062561035,
      "learning_rate": 4.603729603729604e-05,
      "loss": 0.4745,
      "step": 670
    },
    {
      "epoch": 1.285444234404537,
      "grad_norm": 1.540030598640442,
      "learning_rate": 4.5804195804195805e-05,
      "loss": 0.3767,
      "step": 680
    },
    {
      "epoch": 1.3043478260869565,
      "grad_norm": 1.5102111101150513,
      "learning_rate": 4.5571095571095574e-05,
      "loss": 0.4955,
      "step": 690
    },
    {
      "epoch": 1.3232514177693762,
      "grad_norm": 9.48978042602539,
      "learning_rate": 4.533799533799534e-05,
      "loss": 0.5473,
      "step": 700
    },
    {
      "epoch": 1.3421550094517958,
      "grad_norm": 4.218029975891113,
      "learning_rate": 4.5104895104895105e-05,
      "loss": 0.5837,
      "step": 710
    },
    {
      "epoch": 1.3610586011342156,
      "grad_norm": 3.078134059906006,
      "learning_rate": 4.4871794871794874e-05,
      "loss": 0.4556,
      "step": 720
    },
    {
      "epoch": 1.3799621928166352,
      "grad_norm": 6.078334808349609,
      "learning_rate": 4.463869463869464e-05,
      "loss": 0.4982,
      "step": 730
    },
    {
      "epoch": 1.3988657844990549,
      "grad_norm": 15.449082374572754,
      "learning_rate": 4.4405594405594406e-05,
      "loss": 0.4433,
      "step": 740
    },
    {
      "epoch": 1.4177693761814745,
      "grad_norm": 1.2826087474822998,
      "learning_rate": 4.4172494172494175e-05,
      "loss": 0.5726,
      "step": 750
    },
    {
      "epoch": 1.436672967863894,
      "grad_norm": 6.571488380432129,
      "learning_rate": 4.3939393939393944e-05,
      "loss": 0.5509,
      "step": 760
    },
    {
      "epoch": 1.455576559546314,
      "grad_norm": 6.11830472946167,
      "learning_rate": 4.370629370629371e-05,
      "loss": 0.3738,
      "step": 770
    },
    {
      "epoch": 1.4744801512287333,
      "grad_norm": 4.752811431884766,
      "learning_rate": 4.3473193473193475e-05,
      "loss": 0.4444,
      "step": 780
    },
    {
      "epoch": 1.4933837429111532,
      "grad_norm": 4.125759601593018,
      "learning_rate": 4.3240093240093244e-05,
      "loss": 0.4973,
      "step": 790
    },
    {
      "epoch": 1.5122873345935728,
      "grad_norm": 4.825638294219971,
      "learning_rate": 4.300699300699301e-05,
      "loss": 0.5217,
      "step": 800
    },
    {
      "epoch": 1.5311909262759924,
      "grad_norm": 3.1402363777160645,
      "learning_rate": 4.2773892773892776e-05,
      "loss": 0.5259,
      "step": 810
    },
    {
      "epoch": 1.5500945179584122,
      "grad_norm": 5.9964141845703125,
      "learning_rate": 4.254079254079254e-05,
      "loss": 0.4041,
      "step": 820
    },
    {
      "epoch": 1.5689981096408316,
      "grad_norm": 1.508156418800354,
      "learning_rate": 4.230769230769231e-05,
      "loss": 0.5,
      "step": 830
    },
    {
      "epoch": 1.5879017013232515,
      "grad_norm": 2.2417421340942383,
      "learning_rate": 4.2074592074592076e-05,
      "loss": 0.5237,
      "step": 840
    },
    {
      "epoch": 1.606805293005671,
      "grad_norm": 7.557713985443115,
      "learning_rate": 4.1841491841491845e-05,
      "loss": 0.4562,
      "step": 850
    },
    {
      "epoch": 1.6257088846880907,
      "grad_norm": 3.7554588317871094,
      "learning_rate": 4.1608391608391614e-05,
      "loss": 0.5469,
      "step": 860
    },
    {
      "epoch": 1.6446124763705106,
      "grad_norm": 1.824650764465332,
      "learning_rate": 4.1375291375291377e-05,
      "loss": 0.5288,
      "step": 870
    },
    {
      "epoch": 1.66351606805293,
      "grad_norm": 3.647751569747925,
      "learning_rate": 4.1142191142191146e-05,
      "loss": 0.5687,
      "step": 880
    },
    {
      "epoch": 1.6824196597353498,
      "grad_norm": 7.244884967803955,
      "learning_rate": 4.0909090909090915e-05,
      "loss": 0.5113,
      "step": 890
    },
    {
      "epoch": 1.7013232514177694,
      "grad_norm": 2.5205845832824707,
      "learning_rate": 4.067599067599068e-05,
      "loss": 0.4478,
      "step": 900
    },
    {
      "epoch": 1.720226843100189,
      "grad_norm": 6.005762577056885,
      "learning_rate": 4.0442890442890446e-05,
      "loss": 0.5162,
      "step": 910
    },
    {
      "epoch": 1.7391304347826086,
      "grad_norm": 1.5589760541915894,
      "learning_rate": 4.020979020979021e-05,
      "loss": 0.4028,
      "step": 920
    },
    {
      "epoch": 1.7580340264650283,
      "grad_norm": 1.5642635822296143,
      "learning_rate": 3.997668997668998e-05,
      "loss": 0.4451,
      "step": 930
    },
    {
      "epoch": 1.776937618147448,
      "grad_norm": 8.885052680969238,
      "learning_rate": 3.974358974358974e-05,
      "loss": 0.5706,
      "step": 940
    },
    {
      "epoch": 1.7958412098298677,
      "grad_norm": 6.953721523284912,
      "learning_rate": 3.9510489510489516e-05,
      "loss": 0.559,
      "step": 950
    },
    {
      "epoch": 1.8147448015122873,
      "grad_norm": 2.0652401447296143,
      "learning_rate": 3.9277389277389285e-05,
      "loss": 0.3924,
      "step": 960
    },
    {
      "epoch": 1.833648393194707,
      "grad_norm": 5.406424522399902,
      "learning_rate": 3.904428904428905e-05,
      "loss": 0.3882,
      "step": 970
    },
    {
      "epoch": 1.8525519848771266,
      "grad_norm": 5.289506912231445,
      "learning_rate": 3.8811188811188816e-05,
      "loss": 0.5024,
      "step": 980
    },
    {
      "epoch": 1.8714555765595464,
      "grad_norm": 4.752135753631592,
      "learning_rate": 3.857808857808858e-05,
      "loss": 0.3323,
      "step": 990
    },
    {
      "epoch": 1.8903591682419658,
      "grad_norm": 4.641644477844238,
      "learning_rate": 3.834498834498835e-05,
      "loss": 0.6206,
      "step": 1000
    },
    {
      "epoch": 1.9092627599243857,
      "grad_norm": 1.4778504371643066,
      "learning_rate": 3.811188811188811e-05,
      "loss": 0.4234,
      "step": 1010
    },
    {
      "epoch": 1.9281663516068053,
      "grad_norm": 5.170569896697998,
      "learning_rate": 3.787878787878788e-05,
      "loss": 0.4206,
      "step": 1020
    },
    {
      "epoch": 1.947069943289225,
      "grad_norm": 4.539720058441162,
      "learning_rate": 3.764568764568765e-05,
      "loss": 0.5975,
      "step": 1030
    },
    {
      "epoch": 1.9659735349716447,
      "grad_norm": 2.5511326789855957,
      "learning_rate": 3.741258741258741e-05,
      "loss": 0.6142,
      "step": 1040
    },
    {
      "epoch": 1.9848771266540641,
      "grad_norm": 8.227049827575684,
      "learning_rate": 3.717948717948718e-05,
      "loss": 0.5384,
      "step": 1050
    }
  ],
  "logging_steps": 10,
  "max_steps": 2645,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 202225737093120.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
